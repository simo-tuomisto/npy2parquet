{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb53eb72-b44a-47e5-b4e9-9cb7c33f09b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# npy2parquet-module\n",
    "\n",
    "`npy2parquet` is a module that can convert a pandas dataframe that lists numpy arrays into a parquet-file, where all of the numpy data is in one column.\n",
    "\n",
    "One can use the module from the command line, when the input format is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aff6171-37f0-47b4-ab0e-cf3d9a7a1aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: This program will write numpy arrays listed in a CSV into a single parquet file\n",
      "\n",
      "positional arguments:\n",
      "  csv                   CSV that contains the names of the npy files in one\n",
      "                        column\n",
      "  output                Output .parquet-file\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -n NPYFILE_COLUMN, --npyfile-column NPYFILE_COLUMN\n",
      "                        Column that contains the names of the numpy arrays\n",
      "                        (default: npyfile)\n",
      "  -d DATA_COLUMN, --data-column DATA_COLUMN\n",
      "                        Column that will store the numpy array contents\n",
      "                        (default: data)\n",
      "  -b BATCH_SIZE, --batch-size BATCH_SIZE\n",
      "                        How many arrays should be stored in one parquet row-\n",
      "                        group (default: 50)\n",
      "  -s, --shuffle         Shuffle rows before storing the data(default: False)\n",
      "  -r SEED, --seed SEED  Seed used for shuffling (default: 10)\n",
      "  -f, --overwrite       Overwrite output file if it exists (default: False)\n"
     ]
    }
   ],
   "source": [
    "!python -m npy2parquet --help "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabeaf1-94db-40b3-9dcb-90fd219ca895",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This module requires `numpy`-, `pandas`- and `pyarrow`-modules.\n",
    "\n",
    "You can install them with:\n",
    "\n",
    "```sh\n",
    "pip install numpy pandas pyarrow\n",
    "```\n",
    "for virtual environments or\n",
    "```sh\n",
    "conda install numpy pandas pyarrow\n",
    "```\n",
    "for conda environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff3392-aaad-4736-a2fb-7388559b87e7",
   "metadata": {},
   "source": [
    "## Create test numpy arrays\n",
    "\n",
    "Let's create some 10000 test numpy arrays of size 100x100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4501607-04b6-494d-a03c-96122e2b9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 10000 arrays of shape (100, 100)\n",
      "         n    param1    param2                      npyfile\n",
      "0        0  0.215664  0.185119     numpy_arrays/array-0.npy\n",
      "1        1  0.038707  0.180713     numpy_arrays/array-1.npy\n",
      "2        2  0.336700  0.333186     numpy_arrays/array-2.npy\n",
      "3        3  0.866066  0.260158     numpy_arrays/array-3.npy\n",
      "4        4  0.519107  0.106170     numpy_arrays/array-4.npy\n",
      "...    ...       ...       ...                          ...\n",
      "9995  9995  0.919494  0.640358  numpy_arrays/array-9995.npy\n",
      "9996  9996  0.911520  0.739093  numpy_arrays/array-9996.npy\n",
      "9997  9997  0.026405  0.342753  numpy_arrays/array-9997.npy\n",
      "9998  9998  0.789790  0.531555  numpy_arrays/array-9998.npy\n",
      "9999  9999  0.505488  0.449314  numpy_arrays/array-9999.npy\n",
      "\n",
      "[10000 rows x 4 columns]\n",
      "Numpy arrays are described in numpy_arrays.csv\n"
     ]
    }
   ],
   "source": [
    "%run create_test_arrays.py -n 10000 -x 100 -y 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a638a-7cb8-4f82-a25e-118b6855d2fa",
   "metadata": {},
   "source": [
    "The script created a csv-file, where we have a running index (`n`), some hyperparameters (`param1`, `param2`) and the name of the numpy file that is connected to these hyperparameters. It also created 10000 100x100 random numpy arrays and stored them in numpy_arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceeaa997-3db0-430a-8078-d139af2e0f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>param1</th>\n",
       "      <th>param2</th>\n",
       "      <th>npyfile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215664</td>\n",
       "      <td>0.185119</td>\n",
       "      <td>numpy_arrays/array-0.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.180713</td>\n",
       "      <td>numpy_arrays/array-1.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.333186</td>\n",
       "      <td>numpy_arrays/array-2.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.866066</td>\n",
       "      <td>0.260158</td>\n",
       "      <td>numpy_arrays/array-3.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.519107</td>\n",
       "      <td>0.106170</td>\n",
       "      <td>numpy_arrays/array-4.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n    param1    param2                   npyfile\n",
       "0  0  0.215664  0.185119  numpy_arrays/array-0.npy\n",
       "1  1  0.038707  0.180713  numpy_arrays/array-1.npy\n",
       "2  2  0.336700  0.333186  numpy_arrays/array-2.npy\n",
       "3  3  0.866066  0.260158  numpy_arrays/array-3.npy\n",
       "4  4  0.519107  0.106170  numpy_arrays/array-4.npy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('numpy_arrays.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48af43-a0ea-4582-8e13-ae0e04e55b94",
   "metadata": {},
   "source": [
    "## Creating a parquet-file (from command line)\n",
    "\n",
    "Now we can create our parquet-file, where the data in the numpy-arrays is stored in the parquet file with the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103cdd11-bb7e-4aef-94ad-828615992c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset \"numpy_arrays.parquet\" based on \"numpy_arrays.csv\"\n",
      "File numpy_arrays.parquet exists, removing it.\n",
      "Splitting the dataframe with 10000 rows to 500 batches of 20.\n"
     ]
    }
   ],
   "source": [
    "!python -m npy2parquet numpy_arrays.csv numpy_arrays.parquet --npyfile-column npyfile --data-column data --batch-size 20 --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8706786-14b5-4b5c-9e9b-859ea1433057",
   "metadata": {},
   "source": [
    "This creates a parquet file that contains all of the data in the `.npy`-files. Parquet-format stores data in batches, which makes it possible to load files that are larger than our RAM. It also helps with IO.\n",
    "\n",
    "If we want, we can randomize the order of the data before creation. If one needs to access the data in random order (such as in machine learning), it is a good idea to randomize the data order before writing the parquet-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433a546c-076d-4fce-a169-c504b3db901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset \"numpy_arrays.parquet\" based on \"numpy_arrays.csv\"\n",
      "File numpy_arrays.parquet exists, removing it.\n",
      "Shuffling labels with seed 10.\n",
      "Splitting the dataframe with 10000 rows to 500 batches of 20.\n"
     ]
    }
   ],
   "source": [
    "!python -m npy2parquet numpy_arrays.csv numpy_arrays.parquet --npyfile-column npyfile --data-column data --batch-size 20 --shuffle --seed 10 --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea4e99-9237-43da-884f-d6ee7d742f97",
   "metadata": {},
   "source": [
    "Now we can compare the size of the parquet-file to the size of the numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f23a90a-789a-496a-b053-9d50efaa2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782M\tnumpy_arrays\n"
     ]
    }
   ],
   "source": [
    "!du -sh numpy_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a31e4c-e42e-44dd-81af-8d4e9d2b2d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2G\tnumpy_arrays.parquet\n"
     ]
    }
   ],
   "source": [
    "!du -sh numpy_arrays.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18b888-4bbf-4f6b-a99c-909b1cef148b",
   "metadata": {},
   "source": [
    "## Creating a parquet-file (from Python)\n",
    "\n",
    "We can also create the parquet-file from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7845eb-b56b-45a9-aa71-9026e34aaff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File numpy_arrays.parquet exists, removing it.\n",
      "Splitting the dataframe with 10000 rows to 500 batches of 20.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import npy2parquet\n",
    "\n",
    "numpy_df = pd.read_csv('numpy_arrays.csv')\n",
    "\n",
    "npy2parquet.df2parquet(\n",
    "    numpy_df,\n",
    "    'numpy_arrays.parquet',\n",
    "    npyfile_column='npyfile',\n",
    "    data_column='data',\n",
    "    batch_size=20,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3207a-eb9d-4d4a-bb07-fdde1b9c80b6",
   "metadata": {},
   "source": [
    "## Iterating over the parquet-file (sequential order)\n",
    "\n",
    "One way of accessing the parquet-file is through sequential order. We can use the `iter_parquet`-function to iterate over the parquet.\n",
    "\n",
    "What the function does is it loads one batch at a time from the parquet-file, loads it into memory, and returns the data one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d11308-f678-40b8-936b-25dda8dabd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First returned  object is of type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Values of the first object:\n",
      "   n    param1    param2                   npyfile\n",
      "0  0  0.215664  0.185119  numpy_arrays/array-0.npy\n",
      "\n",
      "Second returned object is of type: <class 'numpy.ndarray'>\n",
      "\n",
      "Shape of the second object:\n",
      "(100, 100)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from npy2parquet import iter_parquet\n",
    "for df, data in iter_parquet('numpy_arrays.parquet', data_column='data'):\n",
    "    print(f'First returned  object is of type: {type(df)}\\n')\n",
    "    print(f'Values of the first object:\\n{df}\\n')\n",
    "    print(f'Second returned object is of type: {type(data)}\\n')\n",
    "    print(f'Shape of the second object:\\n{data.shape}\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965859c1-6633-4658-a9f0-bd6720f8c804",
   "metadata": {},
   "source": [
    "We can compare these values to our initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c55a68-0a0e-48c2-961e-83417181902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of our dataframe:\n",
      "   n    param1    param2                   npyfile\n",
      "0  0  0.215664  0.185119  numpy_arrays/array-0.npy\n",
      "\n",
      "Shape of our first numpy array:\n",
      "(100, 100)\n",
      "\n",
      "Data in our numpy arrays match:\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numpy_df = pd.read_csv('numpy_arrays.csv')\n",
    "\n",
    "print(f'First row of our dataframe:\\n{numpy_df.head(1)}\\n')\n",
    "\n",
    "numpy_array_1 = np.load(numpy_df.loc[0,'npyfile'])\n",
    "\n",
    "print(f'Shape of our first numpy array:\\n{numpy_array_1.shape}\\n')\n",
    "\n",
    "print(f'Data in our numpy arrays match:\\n{np.all(numpy_array_1 == data)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b2f5f-ee69-4b11-b447-eaf96ff93ff0",
   "metadata": {},
   "source": [
    "## Iterating over parquet-file (random-order)\n",
    "\n",
    "For random order iteration, we should first create our parquet-file with shuffle enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0faf98-3bcb-4c5a-b66d-41d989e2ea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File numpy_arrays.parquet exists, removing it.\n",
      "Shuffling labels with seed 10.\n",
      "Splitting the dataframe with 10000 rows to 100 batches of 100.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import npy2parquet\n",
    "\n",
    "numpy_df = pd.read_csv('numpy_arrays.csv')\n",
    "\n",
    "npy2parquet.df2parquet(\n",
    "    numpy_df,\n",
    "    'numpy_arrays.parquet',\n",
    "    npyfile_column='npyfile',\n",
    "    data_column='data',\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    seed=10,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6ed12-3a1d-4eaa-8593-42f51a4a8640",
   "metadata": {},
   "source": [
    "Now we can use iter_parquet with shuffling. This is not a complete shuffle, but a pseudorandom shuffle. Basically, the order of the batches will be randomized and the data within a shuffle will be returned in a random order. However, one rarely needs a full random shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394870a4-2acb-4e14-9275-bf3fadecc280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling labels with seed 10.\n",
      "Returned random row:\n",
      "         n    param1    param2                      npyfile\n",
      "6500  7965  0.593723  0.374821  numpy_arrays/array-7965.npy\n"
     ]
    }
   ],
   "source": [
    "for df, data in iter_parquet('numpy_arrays.parquet', data_column='data', shuffle=True, seed=10):\n",
    "    print(f'Returned random row:\\n{df}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092125c-c3cd-4273-9508-5256c2dcf591",
   "metadata": {},
   "source": [
    "Changing the seed will change the batch ordering and ordering within the batch, but it will not recreate the batches, thus it is important to give the dataset a full shuffle while it's being created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b32e6-580c-4cf3-b35e-3e30b6e0db5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Iterating over parquet-file (maximum performance)\n",
    "\n",
    "One can also return named tuples from the iteration, if they are easier to use while iterating (see [itertuples](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples) for more information).\n",
    "\n",
    "This will dramatically increase the iteration speed as the parameters do not need to be converted into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025adedf-5c0d-4938-a2f2-b7304aca96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned random row as a named tuple:\n",
      "Pandas(Index=0, n=5937, param1=0.5332069718172633, param2=0.8945746473961547, npyfile='numpy_arrays/array-5937.npy')\n",
      "Value of param1: 0.5332069718172633\n"
     ]
    }
   ],
   "source": [
    "for param_tuple, data in iter_parquet('numpy_arrays.parquet', data_column='data', return_tuples=True):\n",
    "    print(f'Returned random row as a named tuple:\\n{param_tuple}')\n",
    "    print(f'Value of param1: {param_tuple.param1}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59678846-0ecf-48a7-a842-05b02832fd0e",
   "metadata": {},
   "source": [
    "## Verifying data integrity\n",
    "\n",
    "Storing data in a parquet-file is good, but it is important to make certain that the data is ordered correctly and hyperparameters match the data. The function `verify_parquet` will iterate over the parquet-dataset and it will compare each row of hyperparameters to a given `DataFrame`. In addition, it will compare the data stored in the parquet-file to data stored in the numpy file.\n",
    "\n",
    "Warning: this will read through the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cfe12b0-07c3-4873-be79-e09eafbaa05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran tests for numpy_arrays.parquet:\n",
      "\n",
      "DataFrame tests:\n",
      "Number of rows:                     10000\n",
      "Number of matching rows:            0\n",
      "Number of mismatching rows:         10000\n",
      "\n",
      "Numpy arrays tests:\n",
      "Number of arrays:                   10000\n",
      "Number of matching numpy arrays:    10000\n",
      "Number of mismatching numpy arrays: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from npy2parquet import verify_parquet\n",
    "\n",
    "numpy_arrays_df = pd.read_csv('numpy_arrays.csv')\n",
    "\n",
    "verify_parquet('numpy_arrays.parquet', numpy_arrays_df, npyfile_column='npyfile', data_column='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba7c7f-2e0c-4a2d-90ef-cc1a81d1d195",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "The biggest benefit of using parquet-files comes from the reduced IO. `read_npy_test.py` is a small code that calculates a mean of all of the arrays we have created.\n",
    "\n",
    "Let's use `strace` to measure the number of IO calls created by this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70f0d6c-6ff0-4564-8151-6adcfcd57e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean of 1000 arrays: 0.500112\n",
      "Average mean of 2000 arrays: 0.500070\n",
      "Average mean of 3000 arrays: 0.500084\n",
      "Average mean of 4000 arrays: 0.500044\n",
      "Average mean of 5000 arrays: 0.500022\n",
      "Average mean of 6000 arrays: 0.500018\n",
      "Average mean of 7000 arrays: 0.500031\n",
      "Average mean of 8000 arrays: 0.500041\n",
      "Average mean of 9000 arrays: 0.500048\n",
      "Average mean of 10000 arrays: 0.500045\n",
      "Average mean of arrays: 0.500045\n",
      "% time     seconds  usecs/call     calls    errors syscall\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      " 79,41    0,069900           1     50374           read\n",
      " 19,17    0,016874           1     10273        37 openat\n",
      "  1,30    0,001148           1       774        90 stat\n",
      "  0,05    0,000048           6         7           getcwd\n",
      "  0,04    0,000031           5         6         2 readlink\n",
      "  0,02    0,000019           1        11           write\n",
      "  0,00    0,000000           0         1           lstat\n",
      "  0,00    0,000000           0         1         1 access\n",
      "  0,00    0,000000           0         1           execve\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      "100.00    0,088020                 61448       130 total\n",
      "\n",
      "real\t0m4,068s\n",
      "user\t0m2,366s\n",
      "sys\t0m2,226s\n"
     ]
    }
   ],
   "source": [
    "!time strace -c -e trace=%file,read,write python read_npy_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd7bfd-61da-4976-ad97-ca07a89a9e18",
   "metadata": {},
   "source": [
    "With parquet, the amount of operations is reduced by a significant factor. This is because parquet loads the data one batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45eadd2a-63fd-47cb-9ef9-0b8b91a7395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean of 1000 arrays: 0.499930\n",
      "Average mean of 2000 arrays: 0.500001\n",
      "Average mean of 3000 arrays: 0.500040\n",
      "Average mean of 4000 arrays: 0.500031\n",
      "Average mean of 5000 arrays: 0.500019\n",
      "Average mean of 6000 arrays: 0.500020\n",
      "Average mean of 7000 arrays: 0.500015\n",
      "Average mean of 8000 arrays: 0.500020\n",
      "Average mean of 9000 arrays: 0.500021\n",
      "Average mean of 10000 arrays: 0.500045\n",
      "Average mean of arrays: 0.500045\n",
      "% time     seconds  usecs/call     calls    errors syscall\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      " 43,41    0,002146           0      2216       276 stat\n",
      " 35,69    0,001764           1      1421           read\n",
      " 19,93    0,000985           1       785        80 openat\n",
      "  0,73    0,000036           3        11           write\n",
      "  0,18    0,000009           1         7           getcwd\n",
      "  0,06    0,000003           0         7         3 readlink\n",
      "  0,00    0,000000           0         1           open\n",
      "  0,00    0,000000           0         1           lstat\n",
      "  0,00    0,000000           0         1         1 access\n",
      "  0,00    0,000000           0         1           execve\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      "100.00    0,004943                  4451       360 total\n",
      "\n",
      "real\t0m6,821s\n",
      "user\t0m5,793s\n",
      "sys\t0m2,129s\n"
     ]
    }
   ],
   "source": [
    "!time strace -c -e trace=%file,read,write python read_parquet_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d355d97-78d1-4594-a704-87836b30b49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32dd8b-b14f-49cd-8b10-4138fbe46789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
